{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOEYPGo0bJV3"
   },
   "source": [
    "# Train mobilenet with custom dataset\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SonySemiconductorSolutions/aitrios-rpi-tutorials-ai-model-training/blob/main/notebooks/mobilenet-rps/custom_mobilenet.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial will guide you through the process of creating and training an image classification model for deployment on Raspberry Pi AI Camera.\n",
    "\n",
    "We will normalize the float inputs in preprocessing, and augment our dataset with random preprocessing to avoid overfitting, before training and saving the model as a Keras model. Post-training quantization with [Model Compression Toolkit, MCT](https://github.com/sony/model_optimization/tree/main) developed by Sony Semiconductor Israel.\n",
    "\n",
    "Tutorial includes:\n",
    "- Dataset setup\n",
    "- Model creation\n",
    "- Training\n",
    "- Quantization using [Model Compression Toolkit - MCT](https://github.com/sony/model_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsekigX32vC3",
    "outputId": "23572377-654f-483c-ce3b-f988d23251ea"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtCkDAps6Q98"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dnize3Q6UTF"
   },
   "outputs": [],
   "source": [
    "!pip -q install tensorflow~=2.14.0 model-compression-toolkit~=2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBs0HuDmgRj-"
   },
   "source": [
    "# Settings\n",
    "Letâ€™s set up some variables we can use later when pre-processing the dataset and training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS0Oj8BxgTdY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODELS_DIR = 'models/'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.mkdir(MODELS_DIR)\n",
    "\n",
    "MODEL = MODELS_DIR + 'mobilenet-rps'\n",
    "MODEL_KERAS = MODELS_DIR + 'mobilenet-quant-rps.keras'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SHAPE = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV_TkXYH5HV8"
   },
   "source": [
    "# Dataset\n",
    "We will use the RPS dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUvfbB8i5Fp8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsK6Y9vA_l3O",
    "outputId": "90b4cc1d-3609-4eac-93af-cfa9dc0c47bc"
   },
   "outputs": [],
   "source": [
    "# Load the RPS dataset\n",
    "dataset_name = \"rock_paper_scissors\"\n",
    "(train_ds, validation_ds), info = tfds.load(\n",
    "    name=dataset_name,\n",
    "    split=[\"train\", \"test\"],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(info)\n",
    "\n",
    "class_names = info.features['label'].names\n",
    "num_classes = len(class_names)\n",
    "print(\"class_names:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afX4FS-phtum"
   },
   "source": [
    "## Pre-processing and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XODtqvCTIqQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(image, label):\n",
    "    # Set range\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(tf.cast(image, tf.float32))\n",
    "    # Resize image\n",
    "    image = tf.image.resize(image, IMAGE_SHAPE)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(preprocess_data)\n",
    "validation_ds = validation_ds.map(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Cef4_y5JOUg"
   },
   "outputs": [],
   "source": [
    "# Add augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_ds = validation_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK-F2cUT6kXX"
   },
   "source": [
    "# Keras Mobilenetv2 model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAzbs8AR6mLQ"
   },
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV2\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "base_model =  MobileNetV2(weights='imagenet', include_top=False, input_shape=IMAGE_SHAPE+(3,))\n",
    "\n",
    "# Add custom classification layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(x)\n",
    "\n",
    "# Create the full model\n",
    "float_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "float_model.summary()\n",
    "\"\"\"\n",
    "Top and buttom layers.\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                Output Shape                 Param #   Connected to\n",
    "==================================================================================================\n",
    " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []\n",
    "\n",
    " Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_1[0][0]']\n",
    " ...\n",
    " global_average_pooling2d (  (None, 1280)                 0         ['out_relu[0][0]']\n",
    " GlobalAveragePooling2D)\n",
    "\n",
    " dense (Dense)               (None, 3)                    3843      ['global_average_pooling2d[0][\n",
    "                                                                    0]']\n",
    "\n",
    "==================================================================================================\n",
    "Total params: 2261827 (8.63 MB)\n",
    "Trainable params: 3843 (15.01 KB)\n",
    "Non-trainable params: 2257984 (8.61 MB)\n",
    "__________________________________________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wETcq_pRawe"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KL5p4UFeRdId",
    "outputId": "ec508ce8-79bd-4785-a42d-e45030517e34",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "float_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    baseline=0.8,\n",
    "    min_delta=0.01,\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "\n",
    "history = float_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "float_model.save(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZXhteSU67EZ"
   },
   "source": [
    "# Quantization\n",
    "For full integer quantization, you need to calibrate or estimate the range (Min, Max) of all floating-point tensors in the model. Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers), and model output cannot be calibrated unless we run a few inference cycles. For this, the converter requires a representative dataset to calibrate with. This dataset can be a small subset (around 100-500 samples) of the training or validation data.\n",
    "\n",
    "For details, see [Post-Training Quantization Example of MobileNetV2 Keras Model](https://github.com/sony/model_optimization/blob/v2.0.0/tutorials/notebooks/keras/ptq/example_keras_imagenet.ipynb)\n",
    "\n",
    "Observe that we are using training part of the dataset as representative dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8tayLz769bK"
   },
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "n_iter=10\n",
    "\n",
    "# Create representative dataset generator\n",
    "def get_representative_dataset() -> Generator:\n",
    "    \"\"\"A function that loads the dataset and returns a representative dataset generator.\n",
    "\n",
    "    Returns:\n",
    "        Generator: A generator yielding batches of preprocessed images.\n",
    "    \"\"\"\n",
    "    dataset = train_ds\n",
    "\n",
    "    def representative_dataset() -> Generator:\n",
    "        \"\"\"A generator function that yields batch of preprocessed images.\n",
    "\n",
    "        Yields:\n",
    "            A batch of preprocessed images.\n",
    "        \"\"\"\n",
    "        for _ in range(n_iter):\n",
    "            yield dataset.take(1).get_single_element()[0].numpy()\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "# Create a representative dataset generator\n",
    "representative_dataset_gen = get_representative_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt_nqxAevfvy"
   },
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod\n",
    "\n",
    "# Specify the IMX500-v1 target platform capability (TPC)\n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "\n",
    "# Set the following quantization configurations:\n",
    "# Choose the desired QuantizationErrorMethod for the quantization parameters search.\n",
    "# Enable weights bias correction induced by quantization.\n",
    "# Enable shift negative corrections for improving 'signed' non-linear functions quantization (such as swish, prelu, etc.)\n",
    "# Set the threshold to filter outliers with z_score of 16.\n",
    "q_config = mct.core.QuantizationConfig(activation_error_method=QuantizationErrorMethod.MSE,\n",
    "                                       weights_error_method=QuantizationErrorMethod.MSE,\n",
    "                                       weights_bias_correction=True,\n",
    "                                       shift_negative_activation_correction=True,\n",
    "                                       z_threshold=16)\n",
    "\n",
    "ptq_config = mct.core.CoreConfig(quantization_config=q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCuebCYgvi2Z",
    "outputId": "2a31d56b-7dfa-40e0-8cc0-fb40991a3c92"
   },
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "    in_model=float_model,\n",
    "    representative_data_gen=representative_dataset_gen,\n",
    "    core_config=ptq_config,\n",
    "    target_platform_capabilities=tpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xW8jhSTxDEp",
    "outputId": "ede930fa-7c20-4a7d-b425-4272fe549bae"
   },
   "outputs": [],
   "source": [
    "# Export the quantized model\n",
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path=MODEL_KERAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALDGnqDayC7m"
   },
   "source": [
    "# Visualize detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0-EpSvPApb-",
    "outputId": "145b0424-c7e9-44de-98a0-09f7b84249ab"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load the test part of the dataset\n",
    "test_ds, info = tfds.load(dataset_name, split=[\"test\"], with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WvngiRdyI-y"
   },
   "outputs": [],
   "source": [
    "# Preprocess the input image for inference\n",
    "def preprocess_image_visualization(image):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Perform detection on the input image\n",
    "def detect_objects(model, image):\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    predictions = model.predict(image)\n",
    "    return predictions\n",
    "\n",
    "# Get the class label and confidence score of the detected objects\n",
    "def get_top_prediction(predictions):\n",
    "    top_idx = np.argsort(predictions)[0][-1]\n",
    "    top_score = predictions[0][top_idx]\n",
    "    top_class = class_names[top_idx]\n",
    "    return top_class, top_score\n",
    "\n",
    "# Visualize the detections\n",
    "def visualize_detection(image, cls, score):\n",
    "    plt.imshow(image)\n",
    "    plt.text(10, 20, f'{cls}: {score}', color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "COeW3mCHzX2N",
    "outputId": "326c4d2a-6037-46bd-a57c-bc07d22a62fc"
   },
   "outputs": [],
   "source": [
    "# Visualize detection results for some images\n",
    "for sample in test_ds[0].take(4):\n",
    "    image = preprocess_image_visualization(sample['image'])\n",
    "    predictions = detect_objects(quantized_model, image)  # float_model/quantized_model\n",
    "    cls, score = get_top_prediction(predictions)\n",
    "    print(f'cls: {cls}, score: {score}')\n",
    "    assert score > 0.55\n",
    "    visualize_detection(sample['image'], cls, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "__OBSERVE__: First, save the quantized model to your local machine. You will need it for the conversion and packaging steps.\n",
    "\n",
    "Next step is to convert and package the model for IMX500."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
