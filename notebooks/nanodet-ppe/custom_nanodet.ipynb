{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train nanodet with custom dataset\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SonySemiconductorSolutions/aitrios-rpi-tutorials-ai-model-training/blob/main/notebooks/nanodet-ppe/custom_nanodet.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Training nanodet model to detect Personal Protection Equipment (PPE) using open source dataset. \n",
    "\n",
    "Nanodet training based on https://github.com/RangiLyu/nanodet/tree/main\n",
    "\n",
    "Tutorial includes:\n",
    "- Dataset setup\n",
    "- Nanodet model setup\n",
    "- Training\n",
    "- Quantization using [Model Compression Toolkit - MCT](https://github.com/sony/model_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir torch~=1.13.1 torchvision \"tensorflow>=2.14.0,<2.15.0\" pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial checks in order to continue\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "assert '2.14' in tf.__version__, print(tf.__version__)\n",
    "assert '1.13' in torch.__version__, print(torch.__version__)\n",
    "\n",
    "# Chech shared memory\n",
    "shm_stats = shutil.disk_usage('/dev/shm')\n",
    "shm_in_gb = shm_stats.total / (1024 ** 3)\n",
    "print(f\"shm memory: {shm_in_gb:.2f}GB\")\n",
    "\n",
    "print(f'Is cuda available: {torch.cuda.is_available()}')\n",
    "assert shm_in_gb >= 12 or torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Known errors:\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
    "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
    "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
    "\"\"\"\n",
    "NANODET_COMMIT = 'be9b4a9'\n",
    "!rm -rf nanodet\n",
    "!git clone https://github.com/RangiLyu/nanodet.git\n",
    "!touch nanodet/nanodet/model/__init__.py\n",
    "!cd nanodet && git checkout {NANODET_COMMIT} && pip install -q --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "- go to https://universe.roboflow.com/ai-camp-safety-equipment-detection/ppe-detection-using-cv/dataset/3 and click `\"Download Dataset\"`\n",
    "- choose format `\"COCO\"` and `\"show download code\"` and `\"continue\"`\n",
    "- choose `\"Terminal\"` and copy the command `\"curl...\"` and paste the command in the cell below.\n",
    "- add `\"!\"` in the beginning of the command and replace `\"\\&gt;\"` with `\">\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add below your download code from Roboflow, it should look like the following, with your unique roboflow dataset url:\n",
    "# Example (with \"!\" added in the beginning of the command and replaced \"&gt;\" with \">\". Also added \"-q\" for less output):\n",
    "# !curl -L \"https://universe.roboflow.com/ds/<unique-dataset-url>\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move test/train/valid to dataset folder\n",
    "from pathlib import Path\n",
    "DATASET_PATH = 'dataset/PPE_Detection_Using_CV.v3i.coco'\n",
    "if not Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists():\n",
    "    assert Path(f'train/_annotations.coco.json').exists()\n",
    "    assert Path(f'valid/_annotations.coco.json').exists()\n",
    "    assert Path(f'test/_annotations.coco.json').exists()\n",
    "    !mkdir -p $DATASET_PATH\n",
    "    !mv test train valid *txt $DATASET_PATH/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists()\n",
    "assert Path(f'{DATASET_PATH}/valid/_annotations.coco.json').exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training config file: nanodet-plus-m-1.5x_416-ppe.yml\n",
    "The following block of code creates the nanodet training config file which\n",
    "is based on nanodet/config/nanodet-plus-m-1.5x_416.yml.\n",
    "Updated for the custom PPE dataset\n",
    "Change number of `total_epochs` for better performance.\n",
    "\n",
    "If training on GPU, then set   `gpu_ids`:\n",
    " * 1 gpu: [0]\n",
    " * 2 gpu: [0,1]\n",
    " * etc...\n",
    "\n",
    "Increase `total_epochs`, for example 20.\n",
    "\n",
    "Feel free to increase `val_intervals`, for example 10.\n",
    "\n",
    "For details see nanodet github repo and [config docs](https://github.com/RangiLyu/nanodet/blob/main/docs/config_file_detail.md). Observe recommendation to adjust `lr` with `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch nanodet-plus-m-1.5x_416-ppe.yml\n",
    "cat <<EOF >nanodet-plus-m-1.5x_416-ppe.yml\n",
    "# Comments:\n",
    "# -  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
    "# -  \"device\": settings for colab T4 GPU\n",
    "# -  \"total_epochs\": set to 20 during testing, default 300\n",
    "save_dir: workspace/nanodet-plus-m-1.5x_416-ppe\n",
    "model:\n",
    "  weight_averager:\n",
    "    name: ExpMovingAverager\n",
    "    decay: 0.9998\n",
    "  arch:\n",
    "    name: NanoDetPlus\n",
    "    detach_epoch: 10\n",
    "    backbone:\n",
    "      name: ShuffleNetV2\n",
    "      model_size: 1.5x\n",
    "      out_stages: [2,3,4]\n",
    "      activation: LeakyReLU\n",
    "    fpn:\n",
    "      name: GhostPAN\n",
    "      in_channels: [176, 352, 704]\n",
    "      out_channels: 128\n",
    "      kernel_size: 5\n",
    "      num_extra_level: 1\n",
    "      use_depthwise: True\n",
    "      activation: LeakyReLU\n",
    "    head:\n",
    "      name: NanoDetPlusHead\n",
    "      num_classes: 8\n",
    "      input_channel: 128\n",
    "      feat_channels: 128\n",
    "      stacked_convs: 2\n",
    "      kernel_size: 5\n",
    "      strides: [8, 16, 32, 64]\n",
    "      activation: LeakyReLU\n",
    "      reg_max: 7\n",
    "      norm_cfg:\n",
    "        type: BN\n",
    "      loss:\n",
    "        loss_qfl:\n",
    "          name: QualityFocalLoss\n",
    "          use_sigmoid: True\n",
    "          beta: 2.0\n",
    "          loss_weight: 1.0\n",
    "        loss_dfl:\n",
    "          name: DistributionFocalLoss\n",
    "          loss_weight: 0.25\n",
    "        loss_bbox:\n",
    "          name: GIoULoss\n",
    "          loss_weight: 2.0\n",
    "    # Auxiliary head, only use in training time.\n",
    "    aux_head:\n",
    "      name: SimpleConvHead\n",
    "      num_classes: 8\n",
    "      input_channel: 256\n",
    "      feat_channels: 256\n",
    "      stacked_convs: 4\n",
    "      strides: [8, 16, 32, 64]\n",
    "      activation: LeakyReLU\n",
    "      reg_max: 7\n",
    "data:\n",
    "  train:\n",
    "    name: CocoDataset\n",
    "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/train\n",
    "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json\n",
    "    input_size: [416,416] #[w,h]\n",
    "    keep_ratio: False\n",
    "    pipeline:\n",
    "      perspective: 0.0\n",
    "      scale: [0.6, 1.4]\n",
    "      stretch: [[0.8, 1.2], [0.8, 1.2]]\n",
    "      rotation: 0\n",
    "      shear: 0\n",
    "      translate: 0.2\n",
    "      flip: 0.5\n",
    "      brightness: 0.2\n",
    "      contrast: [0.6, 1.4]\n",
    "      saturation: [0.5, 1.2]\n",
    "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
    "  val:\n",
    "    name: CocoDataset\n",
    "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid\n",
    "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json\n",
    "    input_size: [416,416] #[w,h]\n",
    "    keep_ratio: False\n",
    "    pipeline:\n",
    "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
    "device:\n",
    "  gpu_ids: -1\n",
    "  workers_per_gpu: 2\n",
    "  batchsize_per_gpu: 32\n",
    "  precision: 32 # set to 16 to use AMP training\n",
    "schedule:\n",
    "#  resume:\n",
    "#  load_model:\n",
    "  optimizer:\n",
    "    name: AdamW\n",
    "    lr: 0.001\n",
    "    weight_decay: 0.05\n",
    "  warmup:\n",
    "    name: linear\n",
    "    steps: 500\n",
    "    ratio: 0.0001\n",
    "  total_epochs: 2\n",
    "  lr_schedule:\n",
    "    name: CosineAnnealingLR\n",
    "    T_max: 300\n",
    "    eta_min: 0.00005\n",
    "  val_intervals: 1\n",
    "grad_clip: 35\n",
    "evaluator:\n",
    "  name: CocoDetectionEvaluator\n",
    "  save_key: mAP\n",
    "log:\n",
    "  interval: 10\n",
    "\n",
    "class_names: [\n",
    "  'safety-equipment',\n",
    "  'person',\n",
    "  'goggles',\n",
    "  'helmet',\n",
    "  'no-goggles',\n",
    "  'no-helmet',\n",
    "  'no-vest',\n",
    "  'vest']\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVE: update the following assert statement to match your yml file settings.\n",
    "\n",
    "import yaml\n",
    "with open('nanodet-plus-m-1.5x_416-ppe.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "assert config['device']['gpu_ids'] == -1 or config['device']['gpu_ids'] == [0], print(f\"gpu_ids: {config['device']['gpu_ids']}\")\n",
    "assert config['schedule']['total_epochs'] == 2 or config['schedule']['total_epochs'] == 20, print(f\"total_epochs: {config['schedule']['total_epochs']}\")\n",
    "assert config['schedule']['val_intervals'] == 1 or config['schedule']['val_intervals'] == 10, print(f\"val_intervals: {config['schedule']['val_intervals']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert '1.13' in torch.__version__, print(torch.__version__)\n",
    "assert Path('nanodet-plus-m-1.5x_416-ppe.yml').exists()\n",
    "!export PYTHONPATH=$PWD/nanodet:$PYTHONPATH && python nanodet/tools/train.py nanodet-plus-m-1.5x_416-ppe.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove aux layers that are only used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"./nanodet\")\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from nanodet.model.arch import build_model\n",
    "from nanodet.util import cfg, load_config, Logger\n",
    "\n",
    "def remove_aux(cfg, model_path, remove_layers=['aux_fpn', 'aux_head'], debug=False):\n",
    "    model = build_model(cfg.model)\n",
    "    ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    if len(remove_layers) > 0:\n",
    "        state_dict = copy.deepcopy(ckpt['state_dict'])\n",
    "        for rlayer in remove_layers:\n",
    "            for layer in ckpt['state_dict']:\n",
    "                if rlayer in layer:\n",
    "                    del state_dict[layer]\n",
    "                    if debug:\n",
    "                        print(f'removed layer: {layer}')\n",
    "        del ckpt['state_dict']\n",
    "        ckpt['state_dict'] = copy.deepcopy(state_dict)\n",
    "        del state_dict\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'nanodet-plus-m-1.5x_416-ppe.yml'\n",
    "model_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'\n",
    "dst_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best-removed-aux.pth'\n",
    "\n",
    "load_config(cfg, config_path)\n",
    "ckpt = remove_aux(cfg, model_path, ['aux_fpn', 'aux_head'])\n",
    "torch.save(ckpt, dst_path)\n",
    "print(f'Saved to: {dst_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare size w and w/o aux\n",
    "!ls -l workspace/nanodet-plus-m-1.5x_416-ppe/model_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of custom nanodet model using Model Compression Toolkit\n",
    "Quantization is based on https://github.com/sony/model_optimization/blob/v2.0.0/tutorials/notebooks/keras/ptq/example_keras_nanodet_plus.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir -q \"tensorflow>=2.14.0,<2.15.0\" pycocotools\n",
    "import sys\n",
    "MCT_COMMIT = 'v2.1.0'\n",
    "!rm -rf local_mct\n",
    "!git clone https://github.com/sony/model_optimization.git local_mct && cd local_mct && git checkout {MCT_COMMIT} && pip install --no-cache-dir -r requirements.txt\n",
    "sys.path.insert(0,\"./local_mct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras nanodet float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "assert '2.14' in tf.__version__, print(tf.__version__)\n",
    "assert '1.13' in torch.__version__, print(torch.__version__)\n",
    "\n",
    "from keras.models import Model\n",
    "import model_compression_toolkit as mct\n",
    "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_plus_m\n",
    "from tutorials.mct_model_garden.models_keras.utils.torch2keras_weights_translation import load_state_dict\n",
    "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_box_decoding\n",
    "assert 'local_mct' in mct.__file__, print(mct.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the trained custom model\n",
    "CUSTOM_WEIGHTS_FILE = dst_path  # The nanodet model trained with PPE dataset\n",
    "CLASS_NAMES = [\n",
    "  'safety-equipment',\n",
    "  'person',\n",
    "  'goggles',\n",
    "  'helmet',\n",
    "  'no-goggles',\n",
    "  'no-helmet',\n",
    "  'no-vest',\n",
    "  'vest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "DATASET_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train'\n",
    "ANNOT_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json'\n",
    "DATASET_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid'\n",
    "ANNOT_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json'\n",
    "DATASET_REPR = DATASET_VALID\n",
    "ANNOT_REPR = ANNOT_VALID\n",
    "\n",
    "QUANTIZED_MODEL_NAME = 'nanodet-quant-ppe.keras'\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "N_ITER = 20  # 1 for testing, otherwise 20\n",
    "\n",
    "assert Path(CUSTOM_WEIGHTS_FILE).exists()\n",
    "assert Path(DATASET_REPR).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(weights=CUSTOM_WEIGHTS_FILE, num_classes=NUM_CLASSES):\n",
    "    INPUT_RESOLUTION = 416\n",
    "    INPUT_SHAPE = (INPUT_RESOLUTION, INPUT_RESOLUTION, 3)\n",
    "    SCALE_FACTOR = 1.5\n",
    "    BOTTLENECK_RATIO = 0.5\n",
    "    FEATURE_CHANNELS = 128\n",
    "\n",
    "    pretrained_weights = torch.load(weights, map_location=torch.device('cpu'))['state_dict']\n",
    "    # Generate Nanodet base model\n",
    "    model = nanodet_plus_m(INPUT_SHAPE, SCALE_FACTOR, BOTTLENECK_RATIO, FEATURE_CHANNELS, num_classes)\n",
    "\n",
    "    # Set the pre-trained weights\n",
    "    load_state_dict(model, state_dict_torch=pretrained_weights)\n",
    "\n",
    "    # Add Nanodet Box decoding layer (decode the model outputs to bounding box coordinates)\n",
    "    scores, boxes = nanodet_box_decoding(model.output, res=INPUT_RESOLUTION, num_classes=num_classes)\n",
    "\n",
    "    # Add Tensorflow NMS layer\n",
    "    outputs = tf.image.combined_non_max_suppression(\n",
    "        boxes,\n",
    "        scores,\n",
    "        max_output_size_per_class=300,\n",
    "        max_total_size=300,\n",
    "        iou_threshold=0.65,\n",
    "        score_threshold=0.001,\n",
    "        pad_per_class=False,\n",
    "        clip_boxes=False\n",
    "        )\n",
    "\n",
    "    model = Model(model.input, outputs, name='Nanodet_plus_m_1.5x_416')\n",
    "\n",
    "    print('Model is ready for evaluation')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known warning:  WARNING: head.distribution_project.project not assigned to keras model !!!\n",
    "float_model = get_model(CUSTOM_WEIGHTS_FILE, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTQ quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterator, Tuple, List\n",
    "\n",
    "import cv2\n",
    "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import coco_dataset_generator, CocoEval\n",
    "\n",
    "def nanodet_preprocess(x):\n",
    "    img_mean = [103.53, 116.28, 123.675]\n",
    "    img_std = [57.375, 57.12, 58.395]\n",
    "    x = cv2.resize(x, (416, 416))\n",
    "    x = (x - img_mean) / img_std\n",
    "    return x\n",
    "\n",
    "def get_representative_dataset(n_iter: int, dataset_loader: Iterator[Tuple]):\n",
    "    def representative_dataset() -> Iterator[List]:\n",
    "        ds_iter = iter(dataset_loader)\n",
    "        for _ in range(n_iter):\n",
    "            yield [next(ds_iter)[0]]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "def quantization(float_model, dataset, annot, n_iter=N_ITER):\n",
    "    # Load representative dataset\n",
    "    representative_dataset = coco_dataset_generator(dataset_folder=dataset,\n",
    "                                                    annotation_file=annot,\n",
    "                                                    preprocess=nanodet_preprocess,\n",
    "                                                    batch_size=BATCH_SIZE)\n",
    "\n",
    "    tpc = mct.get_target_platform_capabilities('tensorflow', 'imx500')\n",
    "\n",
    "    # Preform post training quantization\n",
    "    quant_model, _ = mct.ptq.keras_post_training_quantization(\n",
    "        float_model,\n",
    "        representative_data_gen=get_representative_dataset(n_iter, representative_dataset),\n",
    "        target_platform_capabilities=tpc)\n",
    "\n",
    "    print('Quantized model is ready')\n",
    "    return quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model = quantization(float_model, DATASET_REPR, ANNOT_REPR)\n",
    "print(f'Representative dataset: {DATASET_REPR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that loading quantized model might require specification of custom layers,\n",
    "# see https://github.com/sony/model_optimization/issues/1104\n",
    "quant_model.save(QUANTIZED_MODEL_NAME)\n",
    "print(f'Quantized model saved: {QUANTIZED_MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_todo_: coco evaluation of the custom quantized nanodet model requires some update to the mct repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(image_path: str, preprocess: Callable) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load and preprocess an image from a given file path.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        preprocess (function): Preprocessing function to apply to the loaded image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Preprocessed image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = preprocess(image)\n",
    "    image = np.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "# draw a single bounding box onto a numpy array image\n",
    "def draw_bounding_box(img, annotation, scale, class_id, score):\n",
    "    row = scale[0]\n",
    "    col = scale[1]\n",
    "    x_min, y_min = int(annotation[1]*col), int(annotation[0]*row)\n",
    "    x_max, y_max = int(annotation[3]*col), int(annotation[2]*row)\n",
    "\n",
    "    color = (0,255,0)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "    text = f'{int(class_id)}: {score:.2f}'\n",
    "    cv2.putText(img, text, (x_min + 10, y_min + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "# draw all annotation bounding boxes on an image\n",
    "def annotate_image(img, output, scale, quantized_model=False, threshold=0.55):\n",
    "    if quantized_model:\n",
    "        b = output[0].numpy()[0]\n",
    "        s = output[1].numpy()[0]\n",
    "        c = output[2].numpy()[0]\n",
    "    else:\n",
    "        print('Assuming float model')\n",
    "        b = output.nmsed_boxes.numpy()[0]\n",
    "        s = output.nmsed_scores.numpy()[0]\n",
    "        c = output.nmsed_classes.numpy()[0]\n",
    "    for index, row in enumerate(b):\n",
    "        if s[index] > threshold:\n",
    "            #print(f'row: {row}')\n",
    "            id = int(c[index])\n",
    "            draw_bounding_box(img, row, scale, id, s[index])\n",
    "            print(f'class: {CLASS_NAMES[id]} ({id}), score: {s[index]:.2f}')\n",
    "    return {'bbox':b, 'score':s, 'classes':c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See appendix for results. For 2 epochs, the bounding boxes are not perfect...\n",
    "# But improves considerably for 20 epochs.\n",
    "test_img = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/image_257_jpg.rf.1a3a6eb456134cce302712c109645c26.jpg'\n",
    "img = load_and_preprocess_image(f'{test_img}', nanodet_preprocess)\n",
    "output = quant_model(img)\n",
    "image = cv2.imread(f'{test_img}')\n",
    "print(f'image shape: {image.shape}')\n",
    "r = annotate_image(image, output, scale=image.shape, quantized_model=True)\n",
    "assert r['score'][0] > 0.5, print(f\"r['score'][0] > 0.5 failed: {r['score'][0]}\")\n",
    "dst = f'annotated.jpg'\n",
    "if cv2.imwrite(dst, image):\n",
    "    print(f'Annotated image saved to: {dst}')\n",
    "else:\n",
    "    print(f'Failed saving annotated image')\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "__OBSERVE__: First, save the quantized model to your local machine. You will need it for the conversion and packaging steps.\n",
    "\n",
    "Next step is to convert and package the model for IMX500. _todo: link to further instructions. This model requires bgr settings in the post-converter._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## Results total_epochs=2\n",
    "```\n",
    "[NanoDet][07-12 10:47:40]INFO:\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.191\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.026\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.045\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.076\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.100\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.207\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.149\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.271\n",
    "\n",
    "[NanoDet][07-12 10:47:40]INFO:\n",
    "| class            | AP50   | mAP   | class     | AP50   | mAP   |\n",
    "|:-----------------|:-------|:------|:----------|:-------|:------|\n",
    "| Safety-Equipment | nan    | nan   | Person    | 54.1   | 20.1  |\n",
    "| goggles          | 1.6    | 0.4   | helmet    | 40.3   | 13.4  |\n",
    "| no-goggles       | 6.9    | 2.3   | no-helmet | 0.0    | 0.0   |\n",
    "| no-vest          | 3.9    | 1.0   | vest      | 27.1   | 7.9   |\n",
    "[NanoDet][07-12 10:47:40]INFO:Saving model to workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth\n",
    "[NanoDet][07-12 10:47:40]INFO:Val_metrics: {'mAP': 0.06442128900684024, 'AP_50': 0.1912998265318579, 'AP_75': 0.026441697602184976, 'AP_small': 0.010583883892274994, 'AP_m': 0.044976540581496194, 'AP_l': 0.0756733533889817}\n",
    "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
    "```\n",
    "## Results total_epochs=20\n",
    "```\n",
    "Comments:\n",
    "-  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
    "-  \"device\": settings for colab T4 GPU\n",
    "-  \"total_epochs\": set to 20 during testing, default 300\n",
    "...\n",
    "[NanoDet][05-16 09:25:43]INFO:\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.610\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.078\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.362\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.281\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.479\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.185\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.414\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570\n",
    "\n",
    "[NanoDet][05-16 09:25:43]INFO:\n",
    "| class            | AP50   | mAP   | class     | AP50   | mAP   |\n",
    "|:-----------------|:-------|:------|:----------|:-------|:------|\n",
    "| Safety-Equipment | nan    | nan   | Person    | 86.4   | 49.3  |\n",
    "| goggles          | 35.7   | 15.4  | helmet    | 86.0   | 46.3  |\n",
    "| no-goggles       | 42.6   | 16.6  | no-helmet | 34.2   | 13.8  |\n",
    "| no-vest          | 59.6   | 26.3  | vest      | 82.5   | 42.8  |\n",
    "[NanoDet][05-16 09:25:44]INFO:Saving model to workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth\n",
    "[NanoDet][05-16 09:25:44]INFO:Val_metrics: {'mAP': 0.3006027561087712, 'AP_50': 0.6099170448933922, 'AP_75': 0.2496291506747232, 'AP_small': 0.07788513248169772, 'AP_m': 0.212229159695, 'AP_l': 0.36198435595574324}\n",
    "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
    "\n",
    "real\t21m35.722s\n",
    "user\t25m48.699s\n",
    "sys\t6m6.849s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
